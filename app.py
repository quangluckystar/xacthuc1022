# import module
import streamlit as st 
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.preprocessing import LabelEncoder
from underthesea import word_tokenize 
import regex as re
from stopwordsiso import stopwords
#from text_processing_utils import text_preprocess
#from text_processing_utils import colorize_words


############ 2. SETTING UP THE PAGE LAYOUT AND TITLE ############
# Call set_page_config() as the first Streamlit command in your script
st.set_page_config(page_title="Há»‡ thá»‘ng XÃ¡c thá»±c Pháº£n Ã¡nh, Kiáº¿n nghá»‹", page_icon="ğŸ“¢", layout="centered")




with open('styles.css') as f:
    css = f.read()

st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)


# Introduction section
st.sidebar.title('TrÆ°á»ng Äáº¡i Há»c SÃ i GÃ²n')
st.sidebar.header('GVHD: TS.VÅ© Ngá»c Thanh Sang')
st.sidebar.header('HV: LÃª ÄÄƒng Quang')
st.sidebar.markdown("""
- **MÃ£ sá»‘ há»c viÃªn:** CH11221006
- **Lá»›p há»c:** KMTUD221
- **KhÃ³a há»c:** 22.1
""")



# Title
st.title("Há»† THá»NG XÃC THá»°C PHáº¢N ÃNH, KIáº¾N NGHá»Š")

# Xá»­ lÃ½ phÃ¢n loáº¡i vá»›i mÃ´ hÃ¬nh Ä‘Ã£ load tá»« file
MODEL_PATH = "models"
# Load mÃ´ hÃ¬nh  tá»« file
with open(os.path.join(MODEL_PATH, "svm.pkl"), 'rb') as model_file:
    nb_model = pickle.load(model_file)
# Äá»‹nh nghÄ©a label encoder vÃ  khá»Ÿi táº¡o náº¿u cáº§n thiáº¿t
label_encoder = LabelEncoder()
sample_labels = ['__label__PA', '__label__KPA'] #['__label__DGYK', '__label__DL', '__label__KDTT', '__label__Khac', '__label__PA', '__label__TC', '__label__TGTP']    # Example labels
label_encoder.fit(sample_labels)

# # Load mÃ´ hÃ¬nh  tá»« file
# with open(os.path.join(MODEL_PATH, "svm-KPA.pkl"), 'rb') as KPA_model_file:
#     KPA_model = pickle.load(KPA_model_file)
# # Äá»‹nh nghÄ©a label encoder vÃ  khá»Ÿi táº¡o náº¿u cáº§n thiáº¿t
# KPA_label_encoder = LabelEncoder()
# KPA_sample_labels = ['_label__KDTT','__label__GDPL','__label__TC','__label__DGYK','__label__TGTP','__label__Khac','__label__DL'] #['__label__DGYK', '__label__DL', '__label__KDTT', '__label__Khac', '__label__PA', '__label__TC', '__label__TGTP']    # Example labels
# KPA_label_encoder.fit(KPA_sample_labels)


min_length = 50


# Form nháº­p pháº£n há»“i
document = st.text_area('HÃ£y nháº­p ná»™i dung:'.format(min_length), height=180 )
submitted = st.button('Thá»±c hiá»‡n')

# HÃ m dá»± Ä‘oÃ¡n
def predict_label(text):
    # Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Naive Bayes
    label = nb_model.predict([text])[0]
    return label

# HÃ m dá»± Ä‘oÃ¡n
def predict_label_KPA(text):
    # Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Naive Bayes
    label = KPA_model.predict([text])[0]
    return label

 
uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"
 
def loaddicchar():
    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic
dicchar = loaddicchar()

# HÃ m chuyá»ƒn Unicode dá»±ng sáºµn vá» Unicde tá»• há»£p (phá»• biáº¿n hÆ¡n)
def convert_unicode(txt):
    return re.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)

bang_nguyen_am = [['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a'],
                  ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],
                  ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],
                  ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e'],
                  ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],
                  ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i'],
                  ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o'],
                  ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],
                  ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],
                  ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u'],
                  ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],
                  ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y']]
bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']

nguyen_am_to_ids = {}

for i in range(len(bang_nguyen_am)):
    for j in range(len(bang_nguyen_am[i]) - 1):
        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)

def chuan_hoa_dau_tu_tieng_viet(word):
    if not is_valid_vietnam_word(word):
        return word

    chars = list(word)
    dau_cau = 0
    nguyen_am_index = []
    qu_or_gi = False
    for index, char in enumerate(chars):
        x, y = nguyen_am_to_ids.get(char, (-1, -1))
        if x == -1:
            continue
        elif x == 9:  # check qu
            if index != 0 and chars[index - 1] == 'q':
                chars[index] = 'u'
                qu_or_gi = True
        elif x == 5:  # check gi
            if index != 0 and chars[index - 1] == 'g':
                chars[index] = 'i'
                qu_or_gi = True
        if y != 0:
            dau_cau = y
            chars[index] = bang_nguyen_am[x][0]
        if not qu_or_gi or index != 1:
            nguyen_am_index.append(index)
    if len(nguyen_am_index) < 2:
        if qu_or_gi:
            if len(chars) == 2:
                x, y = nguyen_am_to_ids.get(chars[1])
                chars[1] = bang_nguyen_am[x][dau_cau]
            else:
                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))
                if x != -1:
                    chars[2] = bang_nguyen_am[x][dau_cau]
                else:
                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]
            return ''.join(chars)
        return word

    for index in nguyen_am_index:
        x, y = nguyen_am_to_ids[chars[index]]
        if x == 4 or x == 8:  # Ãª, Æ¡
            chars[index] = bang_nguyen_am[x][dau_cau]
            # for index2 in nguyen_am_index:
            #     if index2 != index:
            #         x, y = nguyen_am_to_ids[chars[index]]
            #         chars[index2] = bang_nguyen_am[x][0]
            return ''.join(chars)

    if len(nguyen_am_index) == 2:
        if nguyen_am_index[-1] == len(chars) - 1:
            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]
            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]
            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]
            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]
        else:
            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]
            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]
            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]
            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]
    else:
        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]
        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]
        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]
        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]
        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]
        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]
    return ''.join(chars)


def is_valid_vietnam_word(word):
    chars = list(word)
    nguyen_am_index = -1
    for index, char in enumerate(chars):
        x, y = nguyen_am_to_ids.get(char, (-1, -1))
        if x != -1:
            if nguyen_am_index == -1:
                nguyen_am_index = index
            else:
                if index - nguyen_am_index != 1:
                    return False
                nguyen_am_index = index
    return True


def chuan_hoa_dau_cau_tieng_viet(sentence):
    """
        Chuyá»ƒn cÃ¢u tiáº¿ng viá»‡t vá» chuáº©n gÃµ dáº¥u kiá»ƒu cÅ©.
        :param sentence:
        :return:
        """
    sentence = sentence.lower()
    words = sentence.split()
    for index, word in enumerate(words):
        cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
        # print(cw)
        if len(cw) == 3:
            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])
        words[index] = ''.join(cw)
    return ' '.join(words)

def remove_html(txt):
    return re.sub(r'<[^>]*>', '', txt)

vietnamese_stopwords = stopwords(['vi'])
def remove_stopwords(sentence, stopwords):
    tokens = sentence.split()
    filtered_tokens = [word for word in tokens if word.lower() not in stopwords]
    return ' '.join(filtered_tokens)

def text_preprocess(document):
    # Process each cell in the dataframe to remove stopwords
    document = remove_stopwords(document, vietnamese_stopwords)
    # xÃ³a html code
    document = remove_html(document)
    # chuáº©n hÃ³a unicode
    document = convert_unicode(document)
    # chuáº©n hÃ³a cÃ¡ch gÃµ dáº¥u tiáº¿ng Viá»‡t
    document = chuan_hoa_dau_cau_tieng_viet(document)
    # tÃ¡ch tá»«
    document = word_tokenize(document, format="text")
    # Ä‘Æ°a vá» lower
    document = document.lower()
    # xÃ³a cÃ¡c kÃ½ tá»± khÃ´ng cáº§n thiáº¿t
    document = re.sub(r'[^\s\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘_]',' ',document)
    # xÃ³a khoáº£ng tráº¯ng thá»«a
    document = re.sub(r'\s+', ' ', document).strip()

    #document = remove_whitespace(document)
    return document








processed_document = text_preprocess(document)
st.subheader('Káº¿t quáº£:')
# Xá»­ lÃ½ náº¿u ngÆ°á»i dÃ¹ng Ä‘Ã£ nháº¥n nÃºt PhÃ¢n Loáº¡i
if submitted:
    if document.strip() == '':
        st.warning('Vui lÃ²ng nháº­p ná»™i dung.')
    else:
        # Xá»­ lÃ½ dá»± Ä‘oÃ¡n vÃ  hiá»ƒn thá»‹ káº¿t quáº£
        # Dá»± Ä‘oÃ¡n nhÃ£n
        predicted_label = label_encoder.inverse_transform([predict_label(processed_document)])
        if predicted_label == '__label__PA':
            st.success("Pháº£n Ã¡nh, Kiáº¿n nghá»‹ há»£p lá»‡")
        else:
            st.error("Pháº£n Ã¡nh, Kiáº¿n nghá»‹ khÃ´ng há»£p lá»‡")
             # Dá»± Ä‘oÃ¡n nhÃ£n KPA
           # predicted_label_KPA = KPA_label_encoder.inverse_transform([predict_label_KPA(processed_document)])[0]
            #st.write(predicted_label_KPA)
        #st.write(predicted_label)
      # PhÃ¡t hiá»‡n vÃ  tÃ´ mÃ u tá»« ngá»¯ thÃ´ tá»¥c trong vÄƒn báº£n
    #if processed_document:           
           # result = processed_document
           # st.markdown(f'**Tá»« ngá»¯ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u:**\n{result}', unsafe_allow_html=True)
        #st.write(processed_document)


            
st.markdown("---")

st.header("Danh sÃ¡ch")
uploaded_file = st.file_uploader("Chá»n má»™t file Excel", type=["xlsx", "xls"])

if uploaded_file is not None:
    try:
        df = pd.read_excel(uploaded_file)
     
        st.success("Táº£i file vÃ  Ä‘á»c thÃ nh cÃ´ng.")
        processed_document_row = df["Ná»™i dung pháº£n Ã¡nh"].str.lower()      
        def map_label(text):
            # Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Naive Bayes
            label = nb_model.predict([text_preprocess(text)])[0]
            if label == 0:
                return "KhÃ´ng há»£p lá»‡"
            elif label == 1:
                return "Há»£p lá»‡"
            else:
                return "KhÃ´ng xÃ¡c Ä‘á»‹nh"
        # ThÃªm cá»™t "XÃ¡c thá»±c" vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh (cÃ³ thá»ƒ thay Ä‘á»•i)        
        df['XÃ¡c thá»±c'] = processed_document_row.apply(map_label)
        df = df.drop('STT', axis=1)  # XÃ³a cá»™t 'id'    
      
        st.dataframe(df)
        # Perform any operations on the DataFrame here
    except Exception as e:
        st.error(f"Lá»—i khi Ä‘á»c file: {e}")
else:
    st.warning("KhÃ´ng táº£i Ä‘Æ°á»£c file.")
